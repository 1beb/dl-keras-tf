---
title: "A Few Case Studies to Get Started"
author: "Brad Boehmke"
date: "2020-01-27"
output:
  xaringan::moon_reader:
    css: ["custom.css"]
    self_contained: false
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear, center, middle

background-image: url(images/home4sale.jpg)
background-size: cover

```{r setup, include=FALSE, cache=FALSE}
# set working directory to docs folder
setwd(here::here("docs"))

# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center", 
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)

# Use a clean black and white ggplot2 theme
library(tidyverse)
library(keras)
library(gganimate)
thm <- theme_bw()
theme_set(thm)
```

---
# Vectorization & standardization

.font120.bold[_All inputs and response values in a neural network must be tensors of either 
floating-point or integer data._]

```{r, echo=FALSE}
knitr::include_graphics("images/vectorization.png")
```


---
# Vectorization & standardization

.font120.bold[_Moreover, our feature values should not be relatively large compared to the randomized initial weights <u>and</u> all our features should take values in roughly the same range._]

.pull-left[

- Values should not be significantly larger than the initial weights

- Triggers large gradient updates that will prevent the network from converging

]

--

.pull-right[

- Option 1:
   - standardize between 0-1
   - easy when working with images since all features align to the same range
   
- Option 2:
   - normalize each feature to have mean of 0
   - normalize each feature to have standard deviation of 1
   - common when working with features with different ranges

]

---
# Feature engineering

.pull-left[

* Many different feature engineering techniques to do this plus other great things

* Misperception that neural nets do not require feature engineering


]

.pull-right[

]

---
# Feature engineering

.pull-left[

* Many different feature engineering techniques to do this plus other great things

* Misperception that neural nets do not require feature engineering

* [_Feature Engineering and Selection: A Practical Approach for Predictive Models_](http://www.feat.engineering/) by Max Kuhn & Kjell Johnson


]

.pull-right[

```{r, echo=FALSE, out.height="60%", out.width="60%"}
knitr::include_graphics("https://images.tandf.co.uk/common/jackets/amazon/978113807/9781138079229.jpg")
```

]

---
# Feature engineering

.pull-left[

* Many different feature engineering techniques to do this plus other great things

* Misperception that neural nets do not require feature engineering

* [_Feature Engineering and Selection: A Practical Approach for Predictive Models_](http://www.feat.engineering/) by Max Kuhn & Kjell Johnson

* [_Hands-On Machine Learning with R_](https://bradleyboehmke.github.io/HOML/) by Bradley Boehmke & Brandon Greenwell

]

.pull-right[

```{r, echo=FALSE, out.height="60%", out.width="60%"}
knitr::include_graphics("https://bradleyboehmke.github.io/HOML/images/homl-cover.jpg")
```

]

---
# Ames Example

.pull-left.code70[

```{r, eval=FALSE}
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>% #<<
  step_other(all_nominal(), threshold = .01, other = "other") %>% #<<
  step_integer(matches("(Qual|Cond|QC|Qu)$")) %>%
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

]

.pull-right[

* remove any constant categorical features
* reduce any categorical levels that show in only 1% or less of the observations to a single "other" level

]

---
# Ames Example

.pull-left.code70[

```{r, eval=FALSE}
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>% 
  step_other(all_nominal(), threshold = .01, other = "other") %>% 
  step_integer(matches("(Qual|Cond|QC|Qu)$")) %>% #<<
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

]

.pull-right[

* .bold[Vectorization]: convert features that represent ordered quality metrics to numeric values
   - `Overall_Qual` has 10 Levels: Very_Poor, Poor, Fair, Below_Average, Average, ..., Very_Excellent
   - Converted to: 1, 2, 3, 4, ..., 10

]



---
# Ames Example

.pull-left.code70[

```{r, eval=FALSE}
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>% 
  step_other(all_nominal(), threshold = .01, other = "other") %>% 
  step_integer(matches("(Qual|Cond|QC|Qu)$")) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% #<<
  step_center(all_numeric(), -all_outcomes()) %>% #<<
  step_scale(all_numeric(), -all_outcomes()) %>% #<<
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

]

.pull-right[

* .bold[Standardizes numeric values]

* Yeo-Johnson normalizes value distributions, minimizes outliers which reduces large extreme values

* Centering standardizes features to have mean of zero

* Scaling standardizes feature to have standard deviation of zero

```{r, echo=FALSE, fig.height=2.25}
data.frame(x = rlnorm(1000, 10, 1)) %>%
  mutate(`Regular values` = x,
         `Standardized values` = scale(log(`Regular values`))) %>%
  gather(type, values, -x) %>%
  ggplot(aes(values)) +
    geom_histogram(bins = 50) +
    facet_wrap(~ type, scales = "free_x") +
    scale_x_continuous(NULL, labels = scales::comma)
```


]

---
# Ames Example

.pull-left.code70[

```{r, eval=FALSE}
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>% 
  step_other(all_nominal(), threshold = .01, other = "other") %>% 
  step_integer(matches("(Qual|Cond|QC|Qu)$")) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) #<<
```

]

.pull-right[

* .bold[Vectorize remaining categorical features]

* One-hot encoding

```{r, echo=FALSE}
knitr::include_graphics("https://bradleyboehmke.github.io/HOML/images/ohe-vs-dummy.png")
```

]

---
# Ames Example

.pull-left.code70[

```{r, eval=FALSE}
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>% 
  step_other(all_nominal(), threshold = .01, other = "other") %>% 
  step_integer(matches("(Qual|Cond|QC|Qu)$")) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

]

.pull-right[

```{r, echo=FALSE, out.height="50%", out.width="50%"}
knitr::include_graphics("images/recipes.png")
```

.center[[https://tidymodels.github.io/recipes](https://tidymodels.github.io/recipes/)]

]

---
# Callbacks

.pull-left[

Training a model can be like flying a paper airplane...

<br><br>

...once you let go you have little control over its trajectory!

]

.pull-right[

```{r, echo=FALSE, out.height="80%", out.width="80%"}
knitr::include_graphics("https://media2.giphy.com/media/zMS612WWVzQPu/source.gif")
```

]

---
# Callbacks

.pull-left.font90[

When training a model, sometimes we want to:

<br>

- automatically stop a model once performance has stopped improving

- dynamically adjust values of certain parameters (i.e. learning rate)

- log model information to use or visualize later on

- continually save the model during training and save the model with the best performance

.center[_These tasks and others can help control the trajectory of our model._]

]

---
# Callbacks

.pull-left.font90[

When training a model, sometimes we want to:

<br>

- .blue[automatically stop a model once performance has stopped improving]

- .red[dynamically adjust values of certain parameters (i.e. learning rate)]

- .grey[log model information to use or visualize later on]

- .purple[continually save the model during training and save the model with the best performance]

.center[_These tasks and others can help control the trajectory of our model._]

]

.pull-right.font90[

Callbacks provide a way to control and monitor our model during training:

<br>

- .blue[`callback_early_stopping()`]

- .red[`callback_reduce_lr_on_plateau()`]

- .red[`callback_learning_rate_scheduler()`]

- .grey[`callback_csv_logger()`]

- .purple[`callback_model_checkpoint()`]

- and others (`keras::callback_xxx`)

]

---
# Controlling the size of your network

.pull-left[

___Model capacity___ is controlled by:

* number of layers (_depth_)
* number of nodes (_width_)

<br><br><br>
.center.bold[Typically we see better performance (accuracy & compute efficiency) by increasing the number of layers moreso than nodes]

.center.font80[Rule of `r anicon::faa("thumbs-up", animate = FALSE)` only!]

]

.pull-right[

```{r, echo=FALSE, out.height="90%", out.width="90%"}
knitr::include_graphics(c("images/model_capacity_depth.png",
                          "images/model_capacity_width.png"))
```

]

---
# Best practice for layers & nodes .red[(for preditive models)]

.pull-left[

Layers are typically:

- Tunnel shaped
- Funnel shaped

For best performance:

- Nodes are powers of 2 (16, 32, 64, 128, etc.)
- Relative to number of inputs (remember layers condense our features)
- Consistent number of nodes per layer makes tuning easier
- Last hidden layer should always have more nodes than the output layer



]

.pull-right[

```{r, echo=FALSE, out.height="90%", out.width="90%"}
knitr::include_graphics(c("images/model_capacity_depth.png",
                          "images/model_capacity_funnel.png"))
```

]

---
# Ames Housing

.pull-left[

* Single hidden layer with varying # of neurons


```{r, eval=FALSE}
## # A tibble: 9 x 3
##   neurons min_loss train_time
##     <dbl>    <dbl>      <dbl>
## 1       4   30.5         4.74
## 2       8   37.5         4.61
## 3      16   18.5         4.66
## 4      32   13.9         4.77
## 5      64    8.77        5.06
## 6     128    6.33        5.20
## 7     256    2.80        5.77
## 8     512    1.11        7.25
## 9    1024    0.139       8.75 #<<
```


]

.pull-right[

* Varying # of hidden layers with 128 neurons per layer

```{r, eval=FALSE}
## # A tibble: 8 x 3
##   nlayers min_loss train_time
##     <int>    <dbl>      <dbl>
## 1       1    5.959       6.88
## 2       2    0.824       4.32
## 3       3    0.609       4.32
## 4       4    0.750       4.54
## 5       5    0.665       4.36
## 6       6    0.998       4.43
## 7       7    0.802       4.45
## 8       8    0.603       4.78
```

]


---
class: clear, center, middle

background-image: url(https://www.elitereaders.com/wp-content/uploads/2016/04/worst-movie-reviews-featured.jpg)
background-size: cover
---
# IMDB data set

.pull-left[

```{r, echo=FALSE}
knitr::include_graphics("https://www.wikihow.com/images/thumb/4/46/Prepare-a-Review-on-IMDb-Step-6-Version-2.jpg/aid2512841-v4-728px-Prepare-a-Review-on-IMDb-Step-6-Version-2.jpg")
```


]

.pull-right[

* A  collection of 50,000 reviews from IMDB on the condition there are no more than 30 reviews per movie. 

* The numbers of positive and negative reviews are equal. 
   - .red[Negative] reviews: score $\leq$ 4 out of 10
   - .green[Positive] reviews: score $\geq$ 7 out of 10
   - Neutral reviews are not included. 
   
* The 50,000 reviews are divided evenly into the training and test set.

]

---
# Vectorizing text

.pull-left[

.bold[Before vectorization] $\rightarrow$ list of integers representing words

```{r, echo=FALSE}
imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
```

```{r}
str(train_data)
```


]

.pull-right[

.bold[After vectorization] $\rightarrow$ 2D Tensor of one-hot encoded words

```{r, echo=FALSE}
n_features <- c(train_data, test_data) %>%  
  unlist() %>% 
  max()

# function to create 2D tensor (aka matrix)
vectorize_sequences <- function(sequences, dimension = n_features) {
  # Create a matrix of 0s
  results <- matrix(0, nrow = length(sequences), ncol = dimension)

  # Populate the matrix with 1s
  for (i in seq_along(sequences))
    results[i, sequences[[i]]] <- 1
  results
}

# apply to training and test data
train_data_vec <- vectorize_sequences(train_data)
col_titles <- c("pad", "start", "unknown", paste0("word", 1:(ncol(train_data_vec)-3)))
colnames(train_data_vec) <- col_titles
```

```{r}
train_data_vec[, 1:10]
```

]


---
# Weight regularization

* Regular loss function: $\text{minimize} \left( MSE = \frac{1}{n} \sum^n_{i=1}(Y_i - \hat Y_i)^2 \right)$

* Weight regularization: $\text{minimize} \left( SSE + P \right)$

* $\text{L}^2$ norm (aka _weight decay_): $\text{minimize } \left( MSE + \lambda \sum^p_{j=1} w_j^2 \right)$

<br>

```{r, echo=FALSE, fig.height=4.2, fig.width=9}
boston_train_x <- model.matrix(cmedv ~ ., pdp::boston)[, -1]
boston_train_y <- pdp::boston$cmedv
# model
boston_ridge <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = 0
)
lam <- boston_ridge$lambda %>% 
  as.data.frame() %>%
  mutate(penalty = boston_ridge$a0 %>% names()) %>%
  rename(lambda = ".")
results <- boston_ridge$beta %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lam)
result_labels <- results %>%
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("w", 1:5))

ggplot() +
  geom_line(data = results, aes(lambda, coefficients, group = rowname, color = rowname), 
            show.legend = FALSE) +
  scale_x_log10(breaks = c(1, 7000), labels = c("small", "large")) +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname), 
            nudge_x = -.06, show.legend = FALSE) +
  ylab("weights")
```

---
# Weight regularization

* Regular loss function: $\text{minimize} \left( MSE = \frac{1}{n} \sum^n_{i=1}(Y_i - \hat Y_i)^2 \right)$

* Weight regularization: $\text{minimize} \left( SSE + P \right)$

* $\text{L}^2$ norm (aka _weight decay_): $\text{minimize } \left( MSE + \lambda \sum^p_{j=1} w_j^2 \right)$

* $\text{L}^1$ norm: $\text{minimize } \left( MSE + \lambda \sum^p_{j=1} | w_j | \right)$

```{r, echo=FALSE, fig.height=4, fig.width=9}
# model
boston_lasso <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = 1
)
lam <- boston_lasso$lambda %>% 
  as.data.frame() %>%
  mutate(penalty = boston_lasso$a0 %>% names()) %>%
  rename(lambda = ".")
results <- boston_lasso$beta %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lam)
result_labels <- results %>%
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("x", 1:5))
ggplot() +
  geom_line(data = results, aes(lambda, coefficients, group = rowname, color = rowname), 
            show.legend = FALSE) +
  scale_x_log10(breaks = c(0.01, 7), labels = c("small", "large")) +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname), 
            nudge_x = -.05, show.legend = FALSE) +
  ylab("weights")
```

---
# Dropout

.pull-left[

* During each run, each neuron has *p* probability of being dropped (set to 0)

* Forces more nodes to identify and relay signal in the data

* Helps minimize the model from latching onto happenstance patterns (noise) that are not significant

]

.pull-right[

```{r, echo=FALSE}
knitr::include_graphics("images/dropout_illustration.png")
```


]