---
title: "Project 2: Detecting Duplicate Quora Questions"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
  )
```

This project is designed to test your current knowledge on applying several of
the skills you learned today (i.e. embeddings, LSTM, functional keras API). The
objective is to develop a model that predicts which of the provided pairs of
Quora questions contain the same meaning (could be classified as duplicates).
The dataset first appeared in the Kaggle competition
[Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs) and
consists of approximately 400,000 pairs of questions along with a column
indicating if the question pair is considered a duplicate.

After you complete this project, you can read about Quora's approach to this
problem in this [blog post](https://bit.ly/39po8VI).

___Good luck!___

# Package Requirements

```{r}
library(keras)
library(tidyverse)
library(rsample)
library(testthat)
library(glue)
```

# Import Data

Data can be downloaded from the Kaggle dataset webpage or from Quora’s release
of the dataset. The data set should contain 404,290 observations and 6 columns.

```{r}
quora_data <- get_file(
  "quora_duplicate_questions.tsv",
  "http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv"
) %>%
  read_tsv()

expect_equal(dim(quora_data), c(404290, 6))
```

<br><center>⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️️⚠️⚠️⚠️⚠️⚠️⚠️</center><br>

Often when you are working with large datasets, it is wise to just start with
a fraction of the data to do initial model exploration. Once you have a good
working model, then you can remove this code chunk and run the analysis on the
full dataset. Note that modeling with the full dataset can take multiple hours
on a CPU and even close to an hour on a GPU when using LSTM layers.

<br><center>⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️️⚠️⚠️⚠️⚠️⚠️⚠️</center>

```{r}
# remove after you have a good working model
set.seed(123)
quora_data <- quora_data %>% sample_frac(0.05)
dim(quora_data)
```

Our dataset contains six columns. The last three columns are the ones of
interest:

* __id__: observation (aka row) ID
* __qid1__, __qid2__: unique IDs of each question
* __question1__, __question2__: the full text of each question
* __is_duplicate__: the target variable, set to 1 if question1 and question2
  have essentially the same meaning, and 0 otherwise.

```{r}
quora_data
```

# Data Exploration

We can see that approximately 36% of our question pairs represent questions with
similar meaning (aka duplicates). For this project you do not need to worry
about resampling procedures to balance the response.

```{r}
table(quora_data$is_duplicate) %>% prop.table()
```

Now let's assess our questions text. First, we'll get all unique questions.

```{r}
unique_questions <- unique(c(quora_data$question1, quora_data$question2))
```

Let’s take a look at the number of unique words across all our questions. This
will helps us to decide the vocabular size, which can be treated as a
hyperparameter of our model.

The following code chunk:

1. splits up all our text into individual words,
2. does some text cleaning to remove punctuations and normalize the case,
3. and computes the number of unique words.

We can see that there are 110,842 unique words in the full dataset (23,739
unique words when using our sampled data).

```{r}
unique_questions %>% 
  str_split(pattern = " ") %>% 
  unlist() %>% 
  str_remove_all(pattern = "[[:punct:]]") %>%
  str_to_lower() %>%
  unique() %>% 
  length()
```

Let’s take a look at the number of words in each question. This will helps us to
decide the padding length, another hyperparameter of our model. We can see that
nearly all questions have 32 or less words.

```{r}
unique_questions %>%
  map_int(~ str_count(., "\\w+")) %>%
  quantile(c(0.25, 0.5, 0.75, 0.99), na.rm = TRUE)
```

# Set basic hyperparameters

Let's go ahead and set some basic hyperparameters for our model. These are all
values that you can adjust if time allows.

* __vocab_size__: The size of our vocab. Often, we start with about 50% of our
  total vocab size. So we can start with about 10,000 for our sampled dataset
  and 50,000 for the entire dataset. This would be a hyperparameter you tune
  for optimal performance.
* __max_len__: Length to pad each question to. Since the text is shorter, and
  we want to capture as much content as possible in each question, we can set
  this using the upper quantile (80-95%) of the word distribution, which equates
  to 15-25.
* __embedding_size__: The size of the word embeddings. We'll start large since
  we want to capture fine relationships aross semantic meanings.
* __lstm_size__: The size of the LSTM sequence embedding. Similar to the word
  embedding size, We'll start large since we want to capture fine relationships
  aross semantic meanings.

```{r}
vocab_size <- 10000
max_len <- 20
embedding_size <- 256
lstm_size <- 512
```

# Preprocess our text

Next, let's create a text tokenizer to define how we want to preprocess the text
(i.e. convert to lowercase, remove punctuation, token splitting characters) and
then apply it to our question text.

```{r}
tokenizer <- text_tokenizer(num_words = vocab_size) %>%
  fit_text_tokenizer(unique_questions)
```

Let’s save the tokenizer to disk in order to use it for inference later.

```{r}
save_text_tokenizer(tokenizer, "tokenizer-question-pairs")
```

Next, let's create two new objects:

* `question1`: the text tokenizer for all `quora_data$question1` text
* `question2`: the text tokenizer for all `quora_data$question2` text

```{r}
question1 <- texts_to_sequences(tokenizer, quora_data$question1)
question2 <- texts_to_sequences(tokenizer, quora_data$question2)
```

If you look at the first 6 `question1` obs, we see that they are of different
length. To create embeddings for these questions, we need to standardize their
length.  Go ahead and create two new objects (`question1_padded` & `question2_padded`)
that pad `question1` and `question2`. 

```{r}
question1_padded <- pad_sequences(question1, maxlen = max_len, value = vocab_size + 1)
question2_padded <- pad_sequences(question2, maxlen = max_len, value = vocab_size + 1)
```

# Part 2: Simple benchmark

```{r}
perc_words_question1 <- map2_dbl(question1, question2, ~mean(.x %in% .y))
perc_words_question2 <- map2_dbl(question2, question1, ~ mean(.x %in% .y))

df_model <- data.frame(
  perc_words_question1 = perc_words_question1,
  perc_words_question2 = perc_words_question2,
  is_duplicate = quora_data$is_duplicate
) %>%
  na.omit()
```

```{r}
set.seed(123)
index <- sample.int(nrow(df_model), 0.9 * nrow(df_model))
benchmark_train <- df_model[index, ]
benchmark_valid <- df_model[-index, ]

logistic_regression <- glm(
  is_duplicate ~ perc_words_question1 + perc_words_question2, 
  family = "binomial",
  data = benchmark_train
)

summary(logistic_regression)
```


```{r}
pred <- predict(logistic_regression, benchmark_valid, type = "response")
pred <- pred > mean(benchmark_valid$is_duplicate)
accuracy <- table(pred, benchmark_valid$is_duplicate) %>% 
  prop.table() %>% 
  diag() %>% 
  sum()

glue("Our benchmark model achieves a {round(accuracy * 100, 2)}% accuracy.")
```

# Part 3: Modeling with 

```{r}
# input layers
input_q1 <- layer_input(shape = max_len, name = "Q1")
input_q2 <- layer_input(shape = max_len, name = "Q2")

q1_embeddings <- input_q1 %>% 
  layer_embedding(
    input_dim = vocab_size + 2,
    output_dim = embedding_size,
    input_length = max_len,
    name = "Q1_embeddings"
  ) %>%
  layer_flatten()

q2_embeddings <- input_q2 %>% 
  layer_embedding(
    input_dim = vocab_size + 2,
    output_dim = embedding_size,
    input_length = max_len,
    name = "Q2_embeddings"
  ) %>%
  layer_flatten()

dot <- layer_dot(list(q1_embeddings, q2_embeddings), axes = 1, name = "dot_product")

pred <- dot %>% layer_dense(units = 1, activation = "sigmoid", name = "similarity_prediction")
```

```{r}
model <- keras_model(list(input_q1, input_q2), pred)
model %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = "accuracy"
)

summary(model)
```

```{r}
train_question1_padded <- question1_padded[index,]
train_question2_padded <- question2_padded[index,]
train_response <- quora_data$is_duplicate[index]

val_question1_padded <- question1_padded[-index,]
val_question2_padded <- question2_padded[-index,]
val_response <- quora_data$is_duplicate[-index]
```


```{r}
history <- model %>%
  fit(
    list(train_question1_padded, train_question2_padded),
    train_response, 
    batch_size = 64, 
    epochs = 10, 
    validation_data = list(
      list(val_question1_padded, val_question2_padded), 
      val_response
    ),
    callbacks = list(
      callback_early_stopping(patience = 5),
      callback_reduce_lr_on_plateau(patience = 3)
    )
  )
```

```{r}
predict_question_pairs <- function(model, tokenizer, q1, q2) {
  q1 <- texts_to_sequences(tokenizer, list(q1))
  q2 <- texts_to_sequences(tokenizer, list(q2))
  
  q1 <- pad_sequences(q1, 20)
  q2 <- pad_sequences(q2, 20)
  
  as.numeric(predict(model, list(q1, q2)))
}
```

```{r}
predict_question_pairs(model, tokenizer, "What's R programming?", "What is R programming?")
```

# Part 4: Modeling with 

```{r}
q1_embeddings <- input_q1 %>% 
  layer_embedding(
    input_dim = vocab_size + 2,
    output_dim = embedding_size,
    input_length = max_len,
    embeddings_regularizer = regularizer_l2(0.0001),
    name = "Q1_embeddings"
  )

q2_embeddings <- input_q2 %>% 
  layer_embedding(
    input_dim = vocab_size + 2,
    output_dim = embedding_size,
    input_length = max_len,
    embeddings_regularizer = regularizer_l2(0.0001),
    name = "Q2_embeddings"
  )

q1_lstm <- q1_embeddings %>%
  layer_lstm(
    units = lstm_units, 
    kernel_regularizer = regularizer_l2(0.0001), 
    name = "Q1_lstm"
    )

q2_lstm <- q2_embeddings %>%
  layer_lstm(
    units = lstm_units, 
    kernel_regularizer = regularizer_l2(0.0001),  
    name = "Q2_lstm"
    )

dot <- layer_dot(list(q1_lstm, q2_lstm), axes = 1, name = "dot_product")

pred <- dot %>% layer_dense(units = 1, activation = "sigmoid", name = "similarity_prediction")

model <- keras_model(list(input_q1, input_q2), pred)
model %>% compile(
  optimizer = "rmsprop", 
  loss = "binary_crossentropy", 
  metrics = "accuracy"
)

summary(model)
```

```{r}
history <- model %>%
  fit(
    list(train_question1_padded, train_question2_padded),
    train_response, 
    batch_size = 64, 
    epochs = 10, 
    validation_data = list(
      list(val_question1_padded, val_question2_padded), 
      val_response
    ),
    callbacks = list(
      callback_early_stopping(patience = 5),
      callback_reduce_lr_on_plateau(patience = 3)
    )
  )
```

```{r}
predict_question_pairs(model, tokenizer, "What's R programming?", "What is R programming?")
```
