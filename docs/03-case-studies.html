<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>3 Case Studies to Get Started</title>
    <meta charset="utf-8" />
    <meta name="author" content="Brad Boehmke" />
    <meta name="date" content="2020-01-27" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 3 Case Studies to Get Started
### Brad Boehmke
### 2020-01-27

---

class: clear, center, middle

background-image: url(https://www.buyrentkenya.com/uploadedfiles/ed/5d/ae/ed5daec1-c0fc-4959-9640-97fac7c7f274.jpg)
background-size: cover



---
# Vectorization &amp; standardization

.font120.bold[_All inputs and response values in a neural network must be tensors of either 
floating-point or integer data._]

&lt;img src="images/vectorization.png" width="960" style="display: block; margin: auto;" /&gt;


---
# Vectorization &amp; standardization

.font120.bold[_Moreover, our feature values should not be relatively large compared to the randomized initial weights &lt;u&gt;and&lt;/u&gt; all our features should take values in roughly the same range._]

.pull-left[

- Values should not be significantly larger than the initial weights

- Triggers large gradient updates that will prevent the network from converging

]

--

.pull-right[

- Option 1:
   - standardize between 0-1
   - easy when working with images since all features align to the same range
   
- Option 2:
   - normalize each feature to have mean of 0
   - normalize each feature to have standard deviation of 1
   - common when working with features with different ranges

]

---
# Feature engineering

.pull-left[

* Many different feature engineering techniques to do this plus other great things

* Misperception that neural nets do not require feature engineering


]

.pull-right[

]

---
# Feature engineering

.pull-left[

* Many different feature engineering techniques to do this plus other great things

* Misperception that neural nets do not require feature engineering

* [_Feature Engineering and Selection: A Practical Approach for Predictive Models_](http://www.feat.engineering/) by Max Kuhn &amp; Kjell Johnson


]

.pull-right[

&lt;img src="https://images.tandf.co.uk/common/jackets/amazon/978113807/9781138079229.jpg" width="60%" height="60%" style="display: block; margin: auto;" /&gt;

]

---
# Feature engineering

.pull-left[

* Many different feature engineering techniques to do this plus other great things

* Misperception that neural nets do not require feature engineering

* [_Feature Engineering and Selection: A Practical Approach for Predictive Models_](http://www.feat.engineering/) by Max Kuhn &amp; Kjell Johnson

* [_Hands-On Machine Learning with R_](https://bradleyboehmke.github.io/HOML/) by Bradley Boehmke &amp; Brandon Greenwell

]

.pull-right[

&lt;img src="https://bradleyboehmke.github.io/HOML/images/homl-cover.jpg" width="60%" height="60%" style="display: block; margin: auto;" /&gt;

]

---
# Ames Example

.pull-left.code70[


```r
blueprint &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%
* step_nzv(all_nominal()) %&gt;%
* step_other(all_nominal(), threshold = .01, other = "other") %&gt;%
  step_integer(matches("(Qual|Cond|QC|Qu)$")) %&gt;%
  step_YeoJohnson(all_numeric(), -all_outcomes()) %&gt;%
  step_center(all_numeric(), -all_outcomes()) %&gt;%
  step_scale(all_numeric(), -all_outcomes()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

]

.pull-right[

* remove any constant categorical features
* reduce any categorical levels that show in only 1% or less of the observations to a single "other" level

]

---
# Ames Example

.pull-left.code70[


```r
blueprint &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%
  step_nzv(all_nominal()) %&gt;% 
  step_other(all_nominal(), threshold = .01, other = "other") %&gt;% 
* step_integer(matches("(Qual|Cond|QC|Qu)$")) %&gt;%
  step_YeoJohnson(all_numeric(), -all_outcomes()) %&gt;%
  step_center(all_numeric(), -all_outcomes()) %&gt;%
  step_scale(all_numeric(), -all_outcomes()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

]

.pull-right[

* .bold[Vectorization]: convert features that represent ordered quality metrics to numeric values
   - `Overall_Qual` has 10 Levels: Very_Poor, Poor, Fair, Below_Average, Average, ..., Very_Excellent
   - Converted to: 1, 2, 3, 4, ..., 10

]



---
# Ames Example

.pull-left.code70[


```r
blueprint &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%
  step_nzv(all_nominal()) %&gt;% 
  step_other(all_nominal(), threshold = .01, other = "other") %&gt;% 
  step_integer(matches("(Qual|Cond|QC|Qu)$")) %&gt;% 
* step_YeoJohnson(all_numeric(), -all_outcomes()) %&gt;%
* step_center(all_numeric(), -all_outcomes()) %&gt;%
* step_scale(all_numeric(), -all_outcomes()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

]

.pull-right[

* .bold[Standardizes numeric values]

* Yeo-Johnson normalizes value distributions, minimizes outliers which reduces large extreme values

* Centering standardizes features to have mean of zero

* Scaling standardizes feature to have standard deviation of zero

&lt;img src="03-case-studies_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;


]

---
# Ames Example

.pull-left.code70[


```r
blueprint &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%
  step_nzv(all_nominal()) %&gt;% 
  step_other(all_nominal(), threshold = .01, other = "other") %&gt;% 
  step_integer(matches("(Qual|Cond|QC|Qu)$")) %&gt;% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %&gt;% 
  step_center(all_numeric(), -all_outcomes()) %&gt;%
  step_scale(all_numeric(), -all_outcomes()) %&gt;% 
* step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

]

.pull-right[

* .bold[Vectorize remaining categorical features]

* One-hot encoding

&lt;img src="https://bradleyboehmke.github.io/HOML/images/ohe-vs-dummy.png" style="display: block; margin: auto;" /&gt;

]

---
# Ames Example

.pull-left.code70[


```r
blueprint &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%
  step_nzv(all_nominal()) %&gt;% 
  step_other(all_nominal(), threshold = .01, other = "other") %&gt;% 
  step_integer(matches("(Qual|Cond|QC|Qu)$")) %&gt;% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %&gt;% 
  step_center(all_numeric(), -all_outcomes()) %&gt;%
  step_scale(all_numeric(), -all_outcomes()) %&gt;% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

]

.pull-right[

&lt;img src="https://github.com/rstudio/hex-stickers/blob/master/PNG/recipes.png?raw=true" width="50%" height="50%" style="display: block; margin: auto;" /&gt;

.center[[https://tidymodels.github.io/recipes](https://tidymodels.github.io/recipes/)]

]


---
# Error Metrics

.pull-left.font80[

.bold[MSE]:

* the average of the squared error

* `\(MSE = \frac{1}{n} \sum^n_{i=1}(Y_i - \widehat Y_i)^2\)`

* squared component results in larger errors having larger penalties

]

.pull-right.font80[

House 1:
  - actuals `\(Y = 100,000\)`  
  - predicted: `\(\widehat Y = 90,000\)` 
  - error: `\(Y - \widehat Y = 10,000\)`
  - SE: `\((Y - \widehat Y)^2 = 100,000,000\)` .white[
  - MSLE: 0.1053605]

House 2:
  - actual: `\(Y = 350,000\)`
  - predicted: `\(\widehat Y = 320,000\)`
  - error: `\(Y - \widehat Y = 30,000\)`
  - SE: `\((Y - \widehat Y)^2 = 900,000,000\)` .white[
  - MSLE: 0.08961216]

Total error:
  - MSE: 500,000,000

]

---
# Error Metrics

.pull-left.font80[

.bold[MSE]:

* the average of the squared error

* `\(MSE = \frac{1}{n} \sum^n_{i=1}(Y_i - \hat Y_i)^2\)`

* squared component results in larger errors having larger penalties

.bold[RMSE]:

* commonly used to make error more interpretable

* `\(RMSE = \sqrt{MSE}\)`

]

.pull-right.font80[

House 1:
  - actuals `\(Y = 100,000\)`  
  - predicted: `\(\widehat Y = 90,000\)` 
  - error: `\(Y - \widehat Y = 10,000\)`
  - SE: `\((Y - \widehat Y)^2 = 100,000,000\)` .white[
  - MSLE: 0.1053605]

House 2:
  - actual: `\(Y = 350,000\)`
  - predicted: `\(\widehat Y = 320,000\)`
  - error: `\(Y - \widehat Y = 30,000\)`
  - SE: `\((Y - \widehat Y)^2 = 900,000,000\)` .white[
  - MSLE: 0.08961216]
  
Total error:
  - MSE: 500,000,000 
  - .bold[RMSE]: `\(\sqrt{MSE} = 22,360.68\)`

]

---
# Error Metrics

.pull-left.font80[

.bold[MSE]:

* the average of the squared error

* `\(MSE = \frac{1}{n} \sum^n_{i=1}(Y_i - \hat Y_i)^2\)`

* squared component results in larger errors having larger penalties

.bold[RMSE]:

* commonly used to make error more interpretable

* `\(RMSE = \sqrt{MSE}\)`

.bold[MSLE / RMSLE]:

* adjusts for magnitude of value when you want to treat XX% error equally

* `\(MSE = \frac{1}{n} \sum^n_{i=1}(\log(Y_i) - \log(\hat Y_i))^2\)`

]

.pull-right.font80[

House 1:
  - actuals `\(Y = 100,000\)`  
  - predicted: `\(\widehat Y = 90,000\)` 
  - error: `\(Y - \widehat Y = 10,000\)`
  - SE: `\((Y - \widehat Y)^2 = 100,000,000\)`
  - MSLE: `\((\log(Y_i) - \log(\hat Y_i))^2 = 0.01110084\)`

House 2:
  - actual: `\(Y = 350,000\)`
  - predicted: `\(\widehat Y = 320,000\)`
  - error: `\(Y - \widehat Y = 30,000\)`
  - MSE: `\((Y - \widehat Y)^2 = 900,000,000\)` 
  - MSLE: `\((\log(Y_i) - \log(\hat Y_i))^2 = 0.008030339\)`
  
Total error:
  - MSE: 500,000,000 
  - RMSE: `\(\sqrt{MSE} = 22,360.68\)`
  - .bold[MSLE]: 0.009565589

]

---
# Batch sizes &amp; epochs

* Recall that batch sizes commonly take on values of `\(2^s \rightarrow 32, 64, 128, 256, 512\)`

* And we use enough epochs so that our learning rate reaches a minimum

* General advice:
   - large batch sizes ( `\(\geq 512\)`) tend to reach "sharp minimums" quickly which tend to generalize poorly
   - small batch sizes ( `\(\leq 8\)`) tend to take many more epochs to converge
   - can be influenced by size of data:
      - larger `\(n\)` can afford larger batch sizes (128, 256, 512)
      - smaller `\(n\)` often do better with smaller batch sizes (16, 32, 64)

* Which is best...
   - I typically start with 32 or 64
   - Trial and error for your specific problem


---
# Callbacks

.pull-left[

Training a model can be like flying a paper airplane...

&lt;br&gt;&lt;br&gt;

...once you let go you have little control over its trajectory!

]

.pull-right[

&lt;img src="https://media2.giphy.com/media/zMS612WWVzQPu/source.gif" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

]

---
# Callbacks

.pull-left.font90[

When training a model, sometimes we want to:

&lt;br&gt;

- automatically stop a model once performance has stopped improving

- dynamically adjust values of certain parameters (i.e. learning rate)

- log model information to use or visualize later on

- continually save the model during training and save the model with the best performance

.center[_These tasks and others can help control the trajectory of our model._]

]

---
# Callbacks

.pull-left.font90[

When training a model, sometimes we want to:

&lt;br&gt;

- .blue[automatically stop a model once performance has stopped improving]

- .red[dynamically adjust values of certain parameters (i.e. learning rate)]

- .grey[log model information to use or visualize later on]

- .purple[continually save the model during training and save the model with the best performance]

.center[_These tasks and others can help control the trajectory of our model._]

]

.pull-right.font90[

Callbacks provide a way to control and monitor our model during training:

&lt;br&gt;

- .blue[`callback_early_stopping()`]

- .red[`callback_reduce_lr_on_plateau()`]

- .red[`callback_learning_rate_scheduler()`]

- .grey[`callback_csv_logger()`]

- .purple[`callback_model_checkpoint()`]

- and others (`keras::callback_xxx`)

]

---
# Validation procedures

.pull-left[

So far, we've used `validation_split` to pull out XX% of our training data and use as "unseen" validation data.


```r
network %&gt;% fit(
  x_train,
  y_train,
  epochs = 50,
  batch_size = 32,
* validation_split = 0.2
)
```

]

--

.pull-right[

We can also supply validation data with `validation_data` .white[more text to force on next line ttttttttttttttttt]




```r
network %&gt;% fit(
  x_train,
  y_train,
  epochs = 50,
  batch_size = 32,
* validation_data = list(x_val, y_val)
)
```

]

---
# Validation procedures

.pull-left[

Variability in validation results exists from two sources:

1. randomness in initial weights .white[

2. differences in the actual data used for validation]

]

.pull-right[

&lt;img src="03-case-studies_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

]

---
# Validation procedures

.pull-left[

Variability in validation results exists from two sources:

1. randomness in initial weights

2. differences in the actual data used for validation

]

.pull-right[

&lt;img src="03-case-studies_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;

]

---
# Validation procedures

.pull-left[

Variability in validation results exists from two sources:

1. randomness in initial weights

2. differences in the actual data used for validation

]

.pull-right[

* These difference become minimal for very large data sets

* but can skew our confidence in smaller data sets ( `\(&lt;10\text{K}\)` observations)

]

---
# _k_-fold cross validation

.pull-left[

* _k_-fold CV is a resampling method that randomly divides the training data into _k_ groups (aka folds) of approximately equal size

* fit model on `\(k−1\)` folds and then the remaining fold is used to compute model performance

* average the _k_ error estimates

* typically use `\(k=5\)` or `\(k=10\)`

]

.pull-right[

&lt;img src="https://bradleyboehmke.github.io/HOML/images/cv.png" style="display: block; margin: auto;" /&gt;

]

---
class: clear, center, middle

background-image: url(https://www.elitereaders.com/wp-content/uploads/2016/04/worst-movie-reviews-featured.jpg)
background-size: cover
---
# IMDB data set

.pull-left[

&lt;img src="https://www.wikihow.com/images/thumb/4/46/Prepare-a-Review-on-IMDb-Step-6-Version-2.jpg/aid2512841-v4-728px-Prepare-a-Review-on-IMDb-Step-6-Version-2.jpg" style="display: block; margin: auto;" /&gt;


]

.pull-right[

* A  collection of 50,000 reviews from IMDB on the condition there are no more than 30 reviews per movie. 

* The numbers of positive and negative reviews are equal. 
   - .red[Negative] reviews: score `\(\leq\)` 4 out of 10
   - .green[Positive] reviews: score `\(\geq\)` 7 out of 10
   - Neutral reviews are not included. 
   
* The 50,000 reviews are divided evenly into the training and test set.

]

---
# Vectorizing text

.pull-left[

.bold[Before vectorization] `\(\rightarrow\)` list of integers




```r
str(train_data)
## List of 25000
##  $ : int [1:218] 1 14 22 16 43 530 973 1622 1385 65 ...
##  $ : int [1:189] 1 194 1153 194 8255 78 228 5 6 1463 ...
##  $ : int [1:141] 1 14 47 8 30 31 7 4 249 108 ...
##  $ : int [1:550] 1 4 2 2 33 2804 4 2040 432 111 ...
##  $ : int [1:147] 1 249 1323 7 61 113 10 10 13 1637 ...
##  $ : int [1:43] 1 778 128 74 12 630 163 15 4 1766 ...
##  $ : int [1:123] 1 6740 365 1234 5 1156 354 11 14 5327 ...
##  $ : int [1:562] 1 4 2 716 4 65 7 4 689 4367 ...
##  $ : int [1:233] 1 43 188 46 5 566 264 51 6 530 ...
##  $ : int [1:130] 1 14 20 47 111 439 3445 19 12 15 ...
##  $ : int [1:450] 1 785 189 438 47 110 142 7 6 7475 ...
##  $ : int [1:99] 1 54 13 1610 14 20 13 69 55 364 ...
##  $ : int [1:117] 1 13 119 954 189 1554 13 92 459 48 ...
##  $ : int [1:238] 1 259 37 100 169 1653 1107 11 14 418 ...
##  $ : int [1:109] 1 503 20 33 118 481 302 26 184 52 ...
##  $ : int [1:129] 1 6 964 437 7 58 43 1402 11 6 ...
##  $ : int [1:163] 1 7092 1662 11 4 1749 9 4 2165 4 ...
##  $ : int [1:752] 1 33 4 5673 7 4 2 194 2 3089 ...
##  $ : int [1:212] 1 13 28 64 69 4 8742 7 319 14 ...
##  $ : int [1:177] 1 3432 26 9 6 1220 731 939 44 6 ...
##  $ : int [1:129] 1 617 11 3875 17 2 14 966 78 20 ...
##  $ : int [1:140] 1 466 49 2036 204 2442 40 4 6724 732 ...
##  $ : int [1:256] 1 13 784 886 857 15 135 142 40 2 ...
##  $ : int [1:888] 1 4 712 19 2 2 2 963 2 26 ...
##  $ : int [1:93] 1 4 204 7610 20 16 93 11 9075 19 ...
##  $ : int [1:142] 1 14 9 6 55 641 2854 212 44 6 ...
##  $ : int [1:220] 1 4 288 310 3202 7 241 672 4 2 ...
##  $ : int [1:193] 1 75 69 8 140 8 35 2 38 75 ...
##  $ : int [1:171] 1 4679 2784 1482 11 450 7 134 364 352 ...
##  $ : int [1:221] 1 13 28 332 4 274 6 378 7 211 ...
##  $ : int [1:174] 1 13 1059 14 33 6 2181 2824 32 13 ...
##  $ : int [1:647] 1 6 565 255 5964 875 103 196 167 2 ...
##  $ : int [1:233] 1 4 86 390 1241 520 6 52 1384 51 ...
##  $ : int [1:162] 1 13 92 124 51 12 9 13 169 38 ...
##  $ : int [1:597] 1 111 28 3431 15 2 475 455 4127 9 ...
##  $ : int [1:234] 1 1255 1223 5547 1265 2390 1747 8 4 268 ...
##  $ : int [1:51] 1 806 13 43 161 169 4 875 551 17 ...
##  $ : int [1:336] 1 23 3333 7449 7505 4 254 157 2 7 ...
##  $ : int [1:139] 1 17 19 111 85 1719 1181 3135 201 14 ...
##  $ : int [1:231] 1 14 38 446 1034 9 394 13 435 8 ...
##  $ : int [1:704] 1 14 16 4 840 5582 20 5 4 236 ...
##  $ : int [1:142] 1 14 22 16 23 522 33 314 54 13 ...
##  $ : int [1:861] 1 1710 14 733 1367 1028 81 24 332 48 ...
##  $ : int [1:132] 1 4 229 18 14 248 1844 1422 9 38 ...
##  $ : int [1:122] 1 3420 9 34 230 61 514 7 32 7 ...
##  $ : int [1:570] 1 11 14 4419 3073 14 9061 50 26 1093 ...
##  $ : int [1:55] 1 568 65 9 4689 31 7 4 118 495 ...
##  $ : int [1:214] 1 806 21 13 80 2372 199 4 114 347 ...
##  $ : int [1:103] 1 54 7850 5397 8633 7455 9 2090 2 23 ...
##  $ : int [1:186] 1 13 244 1713 1681 8 97 134 243 7 ...
##  $ : int [1:113] 1 13 165 219 14 20 33 6 750 17 ...
##  $ : int [1:169] 1 159 13 296 14 22 13 332 6 733 ...
##  $ : int [1:469] 1 14 364 1242 2460 2 47 43 77 7548 ...
##  $ : int [1:138] 1 1400 168 855 19 12 50 26 76 128 ...
##  $ : int [1:302] 1 101 20 11 63 7564 2 46 1421 6 ...
##  $ : int [1:766] 1 5011 892 711 12 2774 38 2428 145 11 ...
##  $ : int [1:351] 1 1318 13 191 264 146 4 86 5 64 ...
##  $ : int [1:146] 1 13 974 13 69 8 702 930 143 14 ...
##  $ : int [1:59] 1 13 296 4 20 11 6 4435 5 13 ...
##  $ : int [1:206] 1 209 888 14 22 47 8 30 31 7 ...
##  $ : int [1:107] 1 13 219 14 33 4 2 22 1413 12 ...
##  $ : int [1:152] 1 591 92 851 42 60 104 44 2644 14 ...
##  $ : int [1:186] 1 13 258 14 20 8 30 6 87 326 ...
##  $ : int [1:431] 1 4 2019 9 149 16 35 221 22 585 ...
##  $ : int [1:147] 1 146 242 31 7 4 1126 2406 2266 451 ...
##  $ : int [1:684] 1 1810 8 516 7732 93 6 227 7 6 ...
##  $ : int [1:383] 1 1028 11 86 8213 14 1327 1210 1124 5205 ...
##  $ : int [1:324] 1 2 8810 322 7398 5 68 1667 476 2 ...
##  $ : int [1:252] 1 13 286 1017 76 5 8 30 1202 13 ...
##  $ : int [1:263] 1 48 335 6 337 7 22 1359 5 104 ...
##  $ : int [1:787] 1 6 1380 6733 3453 54 49 432 7 5682 ...
##  $ : int [1:211] 1 14 20 218 290 4 22 12 16 3551 ...
##  $ : int [1:314] 1 49 24 38 2 1028 1404 10 10 138 ...
##  $ : int [1:118] 1 480 302 18 6 20 7 14 58 6 ...
##  $ : int [1:390] 1 670 5304 1616 97 6 20 40 14 21 ...
##  $ : int [1:132] 1 1756 5663 9 2990 2 133 177 17 6 ...
##  $ : int [1:710] 1 1065 2474 7 3508 2 645 113 17 6 ...
##  $ : int [1:306] 1 17 210 14 9 35 6213 431 7 4 ...
##  $ : int [1:167] 1 2500 1040 4 327 1208 44 14 215 28 ...
##  $ : int [1:115] 1 11 4 402 3469 111 7 178 37 69 ...
##  $ : int [1:95] 1 435 8 67 14 17 72 5 61 761 ...
##  $ : int [1:158] 1 31 7 61 118 369 839 14 20 120 ...
##  $ : int [1:156] 1 66 371 2 2 373 21 284 2 2567 ...
##  $ : int [1:82] 1 4 1126 282 13 69 8 67 14 20 ...
##  $ : int [1:502] 1 14 22 7930 236 314 33 2 5510 750 ...
##  $ : int [1:314] 1 13 144 1260 138 13 520 14 418 7 ...
##  $ : int [1:190] 1 13 92 400 140 46 7 61 96 8 ...
##  $ : int [1:174] 1 61 795 203 30 6 227 7 6 1361 ...
##  $ : int [1:60] 1 18 6 20 19 6 114 40 14 13 ...
##  $ : int [1:145] 1 13 219 14 22 5236 145 137 780 23 ...
##  $ : int [1:214] 1 11 192 5192 2 125 2139 1253 7 2 ...
##  $ : int [1:659] 1 541 5156 517 19 4 4791 7 2408 827 ...
##  $ : int [1:408] 1 474 66 66 473 8 67 14 20 5 ...
##  $ : int [1:515] 1 7931 4 425 410 2568 4 876 7 4 ...
##  $ : int [1:461] 1 121 81 13 895 13 473 8 358 14 ...
##  $ : int [1:202] 1 13 66 215 28 1059 6 275 22 39 ...
##  $ : int [1:238] 1 827 2 2 10 10 1251 8598 300 2 ...
##  $ : int [1:170] 1 24 15 76 183 593 11 14 20 21 ...
##  $ : int [1:107] 1 14 20 9 389 10 10 13 16 2 ...
##   [list output truncated]
```


]

.pull-right[

.bold[After vectorization] `\(\rightarrow\)` 2D Tensor of 0s &amp; 1s




```r
train_data_vec[, 1:10]
##          pad start unknown word1 word2 word3 word4 word5 word6 word7
##     [1,]   1     1       0     1     1     1     1     1     1     0
##     [2,]   1     1       0     1     1     1     1     1     1     0
##     [3,]   1     1       0     1     0     1     1     1     1     0
##     [4,]   1     1       0     1     1     1     1     1     1     1
##     [5,]   1     1       0     1     1     1     1     1     0     1
##     [6,]   1     1       0     1     0     0     0     1     0     1
##     [7,]   1     1       0     1     1     1     1     1     1     0
##     [8,]   1     1       0     1     0     1     1     1     1     1
##     [9,]   1     1       0     1     1     1     1     1     1     0
##    [10,]   1     1       0     1     1     1     1     1     1     1
##    [11,]   1     1       0     1     1     1     1     1     1     1
##    [12,]   1     0       0     1     1     1     1     0     1     0
##    [13,]   1     1       0     1     1     1     1     1     1     1
##    [14,]   1     1       0     1     1     1     1     1     1     0
##    [15,]   1     1       0     1     1     1     0     1     1     1
##    [16,]   1     1       0     1     1     1     1     1     0     0
##    [17,]   1     1       0     1     1     1     1     1     1     0
##    [18,]   1     1       0     1     1     1     1     1     1     1
##    [19,]   1     1       0     1     1     1     1     1     1     0
##    [20,]   1     1       0     1     1     1     1     1     1     0
##    [21,]   1     1       0     1     1     1     1     1     1     1
##    [22,]   1     1       0     1     1     1     1     1     1     1
##    [23,]   1     1       0     1     1     1     1     1     1     1
##    [24,]   1     1       0     1     1     1     1     1     1     1
##    [25,]   1     1       0     1     0     1     1     1     1     0
##    [26,]   1     1       0     1     1     1     1     1     1     0
##    [27,]   1     1       0     1     1     1     1     1     1     1
##    [28,]   1     1       0     1     1     1     1     1     1     1
##    [29,]   1     1       0     1     1     1     1     1     1     1
##    [30,]   1     1       0     1     1     1     1     1     1     0
##    [31,]   1     1       0     1     1     1     1     1     1     1
##    [32,]   1     1       0     1     1     1     1     1     1     1
##    [33,]   1     1       0     1     1     1     1     1     1     0
##    [34,]   1     1       0     1     1     1     1     1     1     0
##    [35,]   1     1       0     1     1     1     1     1     1     1
##    [36,]   1     1       0     1     1     1     1     1     1     0
##    [37,]   1     0       0     1     1     1     0     0     0     0
##    [38,]   1     1       0     1     1     1     1     1     1     1
##    [39,]   1     1       0     1     1     1     1     1     1     1
##    [40,]   1     1       0     1     1     1     1     1     1     0
##    [41,]   1     1       0     1     1     1     1     1     1     1
##    [42,]   1     1       0     1     1     1     1     1     1     1
##    [43,]   1     1       0     1     1     1     1     1     1     1
##    [44,]   1     1       0     1     1     1     1     1     1     0
##    [45,]   1     1       0     1     1     1     1     1     1     0
##    [46,]   1     1       0     1     1     1     1     1     1     1
##    [47,]   1     1       0     1     1     0     1     1     1     0
##    [48,]   1     1       0     1     1     1     1     1     1     1
##    [49,]   1     1       0     1     1     1     1     1     1     1
##    [50,]   1     1       0     1     1     1     1     1     1     0
##    [51,]   1     1       0     1     1     1     0     1     0     0
##    [52,]   1     1       0     1     1     1     1     1     1     1
##    [53,]   1     1       0     1     1     1     1     1     1     0
##    [54,]   1     1       0     1     1     1     1     1     1     1
##    [55,]   1     1       0     1     1     1     1     1     1     0
##    [56,]   1     1       0     1     1     1     1     1     1     1
##    [57,]   1     1       0     1     1     1     1     1     1     1
##    [58,]   1     1       0     1     1     1     0     1     1     1
##    [59,]   1     0       0     1     1     1     1     1     1     0
##    [60,]   1     1       0     1     1     0     1     1     1     1
##    [61,]   1     1       0     1     1     1     1     1     0     1
##    [62,]   1     1       0     1     1     1     1     1     1     0
##    [63,]   1     1       0     1     1     1     1     1     1     0
##    [64,]   1     1       0     1     1     1     1     1     1     1
##    [65,]   1     0       0     1     1     1     1     1     1     0
##    [66,]   1     1       0     1     1     1     1     1     1     1
##    [67,]   1     1       0     1     1     1     1     1     1     1
##    [68,]   1     1       0     1     1     1     1     1     1     1
##    [69,]   1     1       0     1     1     1     1     1     1     1
##    [70,]   1     1       0     1     1     1     1     1     1     1
##    [71,]   1     1       0     1     1     1     1     1     1     1
##    [72,]   1     1       0     1     1     1     1     1     1     0
##    [73,]   1     1       0     1     1     1     1     1     1     1
##    [74,]   1     1       0     1     1     1     1     1     1     1
##    [75,]   1     1       0     1     1     1     1     1     1     1
##    [76,]   1     1       0     1     1     1     1     1     1     0
##    [77,]   1     1       0     1     1     1     1     1     1     1
##    [78,]   1     1       0     1     1     1     1     1     1     0
##    [79,]   1     1       0     1     1     1     1     1     1     0
##    [80,]   1     1       0     1     1     1     1     1     1     1
##    [81,]   1     1       0     1     1     1     1     1     1     0
##    [82,]   1     1       0     1     1     1     1     0     1     0
##    [83,]   1     1       0     1     1     1     1     1     0     1
##    [84,]   1     1       0     1     1     1     0     1     0     0
##    [85,]   1     1       0     1     1     1     1     1     1     1
##    [86,]   1     1       0     1     1     1     1     1     1     1
##    [87,]   1     1       0     1     1     1     1     1     1     0
##    [88,]   1     1       0     1     1     1     1     1     1     1
##    [89,]   1     1       0     1     1     1     0     1     0     0
##    [90,]   1     1       0     1     1     1     1     1     1     0
##    [91,]   1     1       0     1     1     1     1     1     1     1
##    [92,]   1     1       0     1     0     1     1     1     1     1
##    [93,]   1     1       0     1     1     1     1     1     1     1
##    [94,]   1     1       0     1     1     1     1     1     1     1
##    [95,]   1     1       0     1     1     1     1     1     1     1
##    [96,]   1     1       0     1     1     1     1     1     1     0
##    [97,]   1     1       0     1     1     1     1     1     1     1
##    [98,]   1     1       0     1     1     1     1     1     1     1
##    [99,]   1     1       0     1     1     1     1     0     1     1
##   [100,]   1     1       0     1     1     1     1     1     1     0
##  [ reached getOption("max.print") -- omitted 24900 rows ]
```

]

---
# Vectorizing text

.pull-left[

list of integers `\(\rightarrow\)` list of word indeces




```r
str(train_data)
## List of 25000
##  $ : chr [1:218] "?" "this" "film" "was" ...
##  $ : chr [1:189] "?" "big" "hair" "big" ...
##  $ : chr [1:141] "?" "this" "has" "to" ...
##  $ : chr [1:550] "?" "the" "?" "?" ...
##  $ : chr [1:147] "?" "worst" "mistake" "of" ...
##  $ : chr [1:43] "?" "begins" "better" "than" ...
##  $ : chr [1:123] "?" "lavish" "production" "values" ...
##  $ : chr [1:562] "?" "the" "?" "tells" ...
##  $ : chr [1:233] "?" "just" "got" "out" ...
##  $ : chr [1:130] "?" "this" "movie" "has" ...
##  $ : chr [1:450] "?" "french" "horror" "cinema" ...
##  $ : chr [1:99] "?" "when" "i" "rented" ...
##  $ : chr [1:117] "?" "i" "love" "cheesy" ...
##  $ : chr [1:238] "?" "anyone" "who" "could" ...
##  $ : chr [1:109] "?" "b" "movie" "at" ...
##  $ : chr [1:129] "?" "a" "total" "waste" ...
##  $ : chr [1:163] "?" "laputa" "castle" "in" ...
##  $ : chr [1:752] "?" "at" "the" "height" ...
##  $ : chr [1:212] "?" "i" "have" "only" ...
##  $ : chr [1:177] "?" "chances" "are" "is" ...
##  $ : chr [1:129] "?" "shown" "in" "australia" ...
##  $ : chr [1:140] "?" "despite" "some" "occasionally" ...
##  $ : chr [1:256] "?" "i" "hate" "reading" ...
##  $ : chr [1:888] "?" "the" "problems" "with" ...
##  $ : chr [1:93] "?" "the" "original" "demille" ...
##  $ : chr [1:142] "?" "this" "is" "a" ...
##  $ : chr [1:220] "?" "the" "dvd" "version" ...
##  $ : chr [1:193] "?" "we" "had" "to" ...
##  $ : chr [1:171] "?" "bela" "lugosi" "appeared" ...
##  $ : chr [1:221] "?" "i" "have" "read" ...
##  $ : chr [1:174] "?" "i" "caught" "this" ...
##  $ : chr [1:647] "?" "a" "strong" "woman" ...
##  $ : chr [1:233] "?" "the" "first" "episode" ...
##  $ : chr [1:162] "?" "i" "don't" "know" ...
##  $ : chr [1:597] "?" "many" "have" "stated" ...
##  $ : chr [1:234] "?" "detective" "tony" "rome" ...
##  $ : chr [1:51] "?" "sorry" "i" "just" ...
##  $ : chr [1:336] "?" "on" "24" "october" ...
##  $ : chr [1:139] "?" "as" "with" "many" ...
##  $ : chr [1:231] "?" "this" "so" "called" ...
##  $ : chr [1:704] "?" "this" "was" "the" ...
##  $ : chr [1:142] "?" "this" "film" "was" ...
##  $ : chr [1:861] "?" "warning" "this" "review" ...
##  $ : chr [1:132] "?" "the" "script" "for" ...
##  $ : chr [1:122] "?" "hamlet" "is" "by" ...
##  $ : chr [1:570] "?" "in" "this" "swimming" ...
##  $ : chr [1:55] "?" "police" "story" "is" ...
##  $ : chr [1:214] "?" "sorry" "but" "i" ...
##  $ : chr [1:103] "?" "when" "philo" "vance" ...
##  $ : chr [1:186] "?" "i" "am" "rarely" ...
##  $ : chr [1:113] "?" "i" "actually" "saw" ...
##  $ : chr [1:169] "?" "before" "i" "watched" ...
##  $ : chr [1:469] "?" "this" "low" "grade" ...
##  $ : chr [1:138] "?" "hey" "look" "deal" ...
##  $ : chr [1:302] "?" "any" "movie" "in" ...
##  $ : chr [1:766] "?" "kubrick" "meets" "king" ...
##  $ : chr [1:351] "?" "wow" "i" "can't" ...
##  $ : chr [1:146] "?" "i" "admit" "i" ...
##  $ : chr [1:59] "?" "i" "watched" "the" ...
##  $ : chr [1:206] "?" "without" "question" "this" ...
##  $ : chr [1:107] "?" "i" "saw" "this" ...
##  $ : chr [1:152] "?" "please" "don't" "rent" ...
##  $ : chr [1:186] "?" "i" "found" "this" ...
##  $ : chr [1:431] "?" "the" "sea" "is" ...
##  $ : chr [1:147] "?" "i'm" "probably" "one" ...
##  $ : chr [1:684] "?" "dressed" "to" "kill" ...
##  $ : chr [1:383] "?" "spoilers" "in" "first" ...
##  $ : chr [1:324] "?" "?" "clutter" "wife" ...
##  $ : chr [1:252] "?" "i" "wasn't" "expecting" ...
##  $ : chr [1:263] "?" "if" "you're" "a" ...
##  $ : chr [1:787] "?" "a" "stunning" "realization" ...
##  $ : chr [1:211] "?" "this" "movie" "isn't" ...
##  $ : chr [1:314] "?" "some" "not" "so" ...
##  $ : chr [1:118] "?" "amazing" "effects" "for" ...
##  $ : chr [1:390] "?" "robert" "altman" "shouldn't" ...
##  $ : chr [1:132] "?" "tim" "robbins" "is" ...
##  $ : chr [1:710] "?" "'the" "adventures" "of" ...
##  $ : chr [1:306] "?" "as" "always" "this" ...
##  $ : chr [1:167] "?" "whoever" "wrote" "the" ...
##  $ : chr [1:115] "?" "in" "the" "early" ...
##  $ : chr [1:95] "?" "went" "to" "see" ...
##  $ : chr [1:158] "?" "one" "of" "my" ...
##  $ : chr [1:156] "?" "really" "truly" "?" ...
##  $ : chr [1:82] "?" "the" "biggest" "reason" ...
##  $ : chr [1:502] "?" "this" "film" "screened" ...
##  $ : chr [1:314] "?" "i" "should" "explain" ...
##  $ : chr [1:190] "?" "i" "don't" "often" ...
##  $ : chr [1:174] "?" "my" "comments" "may" ...
##  $ : chr [1:60] "?" "for" "a" "movie" ...
##  $ : chr [1:145] "?" "i" "saw" "this" ...
##  $ : chr [1:214] "?" "in" "fact" "marc" ...
##  $ : chr [1:659] "?" "blood" "legacy" "starts" ...
##  $ : chr [1:408] "?" "i'd" "really" "really" ...
##  $ : chr [1:515] "?" "tashan" "the" "title" ...
##  $ : chr [1:461] "?" "where" "do" "i" ...
##  $ : chr [1:202] "?" "i" "really" "must" ...
##  $ : chr [1:238] "?" "tom" "?" "?" ...
##  $ : chr [1:170] "?" "not" "that" "much" ...
##  $ : chr [1:107] "?" "this" "movie" "is" ...
##   [list output truncated]
```


]

.pull-right[

2D Tensor of 0s &amp; 1s `\(\rightarrow\)` one-hot encoded words





```r
train_data_vec[, 1:10]
##          pad start unknown the and a of to is br
##     [1,]   1     1       0   1   1 1  1  1  1  0
##     [2,]   1     1       0   1   1 1  1  1  1  0
##     [3,]   1     1       0   1   0 1  1  1  1  0
##     [4,]   1     1       0   1   1 1  1  1  1  1
##     [5,]   1     1       0   1   1 1  1  1  0  1
##     [6,]   1     1       0   1   0 0  0  1  0  1
##     [7,]   1     1       0   1   1 1  1  1  1  0
##     [8,]   1     1       0   1   0 1  1  1  1  1
##     [9,]   1     1       0   1   1 1  1  1  1  0
##    [10,]   1     1       0   1   1 1  1  1  1  1
##    [11,]   1     1       0   1   1 1  1  1  1  1
##    [12,]   1     0       0   1   1 1  1  0  1  0
##    [13,]   1     1       0   1   1 1  1  1  1  1
##    [14,]   1     1       0   1   1 1  1  1  1  0
##    [15,]   1     1       0   1   1 1  0  1  1  1
##    [16,]   1     1       0   1   1 1  1  1  0  0
##    [17,]   1     1       0   1   1 1  1  1  1  0
##    [18,]   1     1       0   1   1 1  1  1  1  1
##    [19,]   1     1       0   1   1 1  1  1  1  0
##    [20,]   1     1       0   1   1 1  1  1  1  0
##    [21,]   1     1       0   1   1 1  1  1  1  1
##    [22,]   1     1       0   1   1 1  1  1  1  1
##    [23,]   1     1       0   1   1 1  1  1  1  1
##    [24,]   1     1       0   1   1 1  1  1  1  1
##    [25,]   1     1       0   1   0 1  1  1  1  0
##    [26,]   1     1       0   1   1 1  1  1  1  0
##    [27,]   1     1       0   1   1 1  1  1  1  1
##    [28,]   1     1       0   1   1 1  1  1  1  1
##    [29,]   1     1       0   1   1 1  1  1  1  1
##    [30,]   1     1       0   1   1 1  1  1  1  0
##    [31,]   1     1       0   1   1 1  1  1  1  1
##    [32,]   1     1       0   1   1 1  1  1  1  1
##    [33,]   1     1       0   1   1 1  1  1  1  0
##    [34,]   1     1       0   1   1 1  1  1  1  0
##    [35,]   1     1       0   1   1 1  1  1  1  1
##    [36,]   1     1       0   1   1 1  1  1  1  0
##    [37,]   1     0       0   1   1 1  0  0  0  0
##    [38,]   1     1       0   1   1 1  1  1  1  1
##    [39,]   1     1       0   1   1 1  1  1  1  1
##    [40,]   1     1       0   1   1 1  1  1  1  0
##    [41,]   1     1       0   1   1 1  1  1  1  1
##    [42,]   1     1       0   1   1 1  1  1  1  1
##    [43,]   1     1       0   1   1 1  1  1  1  1
##    [44,]   1     1       0   1   1 1  1  1  1  0
##    [45,]   1     1       0   1   1 1  1  1  1  0
##    [46,]   1     1       0   1   1 1  1  1  1  1
##    [47,]   1     1       0   1   1 0  1  1  1  0
##    [48,]   1     1       0   1   1 1  1  1  1  1
##    [49,]   1     1       0   1   1 1  1  1  1  1
##    [50,]   1     1       0   1   1 1  1  1  1  0
##    [51,]   1     1       0   1   1 1  0  1  0  0
##    [52,]   1     1       0   1   1 1  1  1  1  1
##    [53,]   1     1       0   1   1 1  1  1  1  0
##    [54,]   1     1       0   1   1 1  1  1  1  1
##    [55,]   1     1       0   1   1 1  1  1  1  0
##    [56,]   1     1       0   1   1 1  1  1  1  1
##    [57,]   1     1       0   1   1 1  1  1  1  1
##    [58,]   1     1       0   1   1 1  0  1  1  1
##    [59,]   1     0       0   1   1 1  1  1  1  0
##    [60,]   1     1       0   1   1 0  1  1  1  1
##    [61,]   1     1       0   1   1 1  1  1  0  1
##    [62,]   1     1       0   1   1 1  1  1  1  0
##    [63,]   1     1       0   1   1 1  1  1  1  0
##    [64,]   1     1       0   1   1 1  1  1  1  1
##    [65,]   1     0       0   1   1 1  1  1  1  0
##    [66,]   1     1       0   1   1 1  1  1  1  1
##    [67,]   1     1       0   1   1 1  1  1  1  1
##    [68,]   1     1       0   1   1 1  1  1  1  1
##    [69,]   1     1       0   1   1 1  1  1  1  1
##    [70,]   1     1       0   1   1 1  1  1  1  1
##    [71,]   1     1       0   1   1 1  1  1  1  1
##    [72,]   1     1       0   1   1 1  1  1  1  0
##    [73,]   1     1       0   1   1 1  1  1  1  1
##    [74,]   1     1       0   1   1 1  1  1  1  1
##    [75,]   1     1       0   1   1 1  1  1  1  1
##    [76,]   1     1       0   1   1 1  1  1  1  0
##    [77,]   1     1       0   1   1 1  1  1  1  1
##    [78,]   1     1       0   1   1 1  1  1  1  0
##    [79,]   1     1       0   1   1 1  1  1  1  0
##    [80,]   1     1       0   1   1 1  1  1  1  1
##    [81,]   1     1       0   1   1 1  1  1  1  0
##    [82,]   1     1       0   1   1 1  1  0  1  0
##    [83,]   1     1       0   1   1 1  1  1  0  1
##    [84,]   1     1       0   1   1 1  0  1  0  0
##    [85,]   1     1       0   1   1 1  1  1  1  1
##    [86,]   1     1       0   1   1 1  1  1  1  1
##    [87,]   1     1       0   1   1 1  1  1  1  0
##    [88,]   1     1       0   1   1 1  1  1  1  1
##    [89,]   1     1       0   1   1 1  0  1  0  0
##    [90,]   1     1       0   1   1 1  1  1  1  0
##    [91,]   1     1       0   1   1 1  1  1  1  1
##    [92,]   1     1       0   1   0 1  1  1  1  1
##    [93,]   1     1       0   1   1 1  1  1  1  1
##    [94,]   1     1       0   1   1 1  1  1  1  1
##    [95,]   1     1       0   1   1 1  1  1  1  1
##    [96,]   1     1       0   1   1 1  1  1  1  0
##    [97,]   1     1       0   1   1 1  1  1  1  1
##    [98,]   1     1       0   1   1 1  1  1  1  1
##    [99,]   1     1       0   1   1 1  1  0  1  1
##   [100,]   1     1       0   1   1 1  1  1  1  0
##  [ reached getOption("max.print") -- omitted 24900 rows ]
```

]


---
# Sigmoid activation

.pull-left[

* Recall how sigmoid activation gives us a 0-1 probability

* In this case, we are predicting the probability that a review is positive.

]

.pull-right.center[

&lt;img src="03-case-studies_files/figure-html/unnamed-chunk-27-1.png" style="display: block; margin: auto;" /&gt;

.font120[
`\(f(y) = \frac{1}{1 + e^{-y}}\)`
]
]

---
# Binary crossentropy

.pull-left[

* Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. 

* Cross-entropy loss increases as the predicted probability diverges from the actual label. 

* So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value

]

.pull-right[

&lt;img src="03-case-studies_files/figure-html/unnamed-chunk-28-1.png" style="display: block; margin: auto;" /&gt;

`$$-{(y \cdot \log(p) + (1 - y) \cdot \log(1 - p))}$$`

]

---
# Binary crossentropy

.pull-left[

* Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. 

* Cross-entropy loss increases as the predicted probability diverges from the actual label. 

* So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value

* Binary crossentropy is just the average of these values across all observations

`$$-\frac{1}{N} \sum^{N=2}_{i=1} {(y_i \cdot \log(p_i) + (1 - y_i) \cdot \log(1 - p_i))}$$`

]

.pull-right[

&lt;img src="03-case-studies_files/figure-html/unnamed-chunk-29-1.png" style="display: block; margin: auto;" /&gt;

`$$-{(y \cdot \log(p) + (1 - y) \cdot \log(1 - p))}$$`

]

---
# Customizing compile step

.pull-left.font100[

* The compile step allows us to specify how gradient descent will be deployed and the metrics to track

* This can be customized multiple ways...

]

.pull-right[


```r
network %&gt;% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "accuracy",
  loss_weights = NULL, 
  sample_weight_mode = NULL,
  weighted_metrics = NULL, 
  target_tensors = NULL, 
  ...
)
```

]

---
# Customizing compile step

.pull-left.font100[

* The compile step allows us to specify how gradient descent will be deployed and the metrics to track

* This can be customized multiple ways...
   - We can customize our optimizer function (typically we only alter the learning rate)

]

.pull-right[


```r
network %&gt;% compile(
* optimizer =  optimizer_sgd(lr = 0.005),
  loss = "binary_crossentropy",
  metrics = "accuracy",
  loss_weights = NULL, 
  sample_weight_mode = NULL,
  weighted_metrics = NULL, 
  target_tensors = NULL, 
  ...
)
```

]

---
# Customizing compile step

.pull-left.font100[

* The compile step allows us to specify how gradient descent will be deployed and the metrics to track

* This can be customized multiple ways...
   - We can customize our optimizer function
   - We can create customized loss functions and metrics to pass

]

.pull-right[


```r
# create metric using backend tensor functions
*metric_mean_pred &lt;- custom_metric("mean_pred", function(y_true, y_pred) {
* k_mean(y_pred)
*})

network %&gt;% compile(
  optimizer =  optimizer_sgd(lr = 0.005),
* loss = loss_binary_crossentropy,
* metrics = c("accuracy", metric_mean_pred)
  loss_weights = NULL, 
  sample_weight_mode = NULL,
  weighted_metrics = NULL, 
  target_tensors = NULL, 
  ...
)
```

]

---
# Customizing compile step

.pull-left.font100[

* The compile step allows us to specify how gradient descent will be deployed and the metrics to track

* This can be customized multiple ways...
   - We can customize our optimizer function
   - We can create customized loss functions and metrics to pass
   - We can include:
      - weighted loss contributions
      - weight metrics
      - and more sophisticated control options

]

.pull-right[


```r
network %&gt;% compile(
  optimizer =  optimizer_sgd(lr = 0.005),
  loss = loss_binary_crossentropy, 
  metrics = c("accuracy", metric_mean_pred) 
* loss_weights = NULL,
* sample_weight_mode = NULL,
* weighted_metrics = NULL,
* target_tensors = NULL,
* ...
)
```

]

---
# Controlling the size of your network

.pull-left[

___Model capacity___ is controlled by:

* number of layers (_depth_)
* number of nodes (_width_)

&lt;br&gt;&lt;br&gt;&lt;br&gt;
.center.bold[Typically we see better performance (accuracy &amp; compute efficiency) by increasing the number of layers moreso than nodes]

]

.pull-right[

&lt;img src="images/model_capacity_depth.png" width="90%" height="90%" style="display: block; margin: auto;" /&gt;&lt;img src="images/model_capacity_width.png" width="90%" height="90%" style="display: block; margin: auto;" /&gt;

]

---
# Best practice for layers &amp; nodes .red[(for preditive models)]

.pull-left[

Layers are typically:

- Tunnel shaped
- Funnel shaped

For best performance:

- Nodes are powers of 2 (16, 32, 64, 128, etc.)
- Relative to number of inputs (remember layers condense our features)
- Consistent number of nodes per layer makes tuning easier
- Last hidden layer should always have more nodes than the output layer



]

.pull-right[

&lt;img src="images/model_capacity_depth.png" width="90%" height="90%" style="display: block; margin: auto;" /&gt;&lt;img src="images/model_capacity_funnel.png" width="90%" height="90%" style="display: block; margin: auto;" /&gt;

]

---
# Ames Housing

.pull-left[

* Single hidden layer with varying # of neurons



```r
## # A tibble: 9 x 3
##   neurons min_loss train_time
##     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
## 1       4   30.5         4.74
## 2       8   37.5         4.61
## 3      16   18.5         4.66
## 4      32   13.9         4.77
## 5      64    8.77        5.06
## 6     128    6.33        5.20
## 7     256    2.80        5.77
## 8     512    1.11        7.25
## 9    1024    0.139       8.75
```


]

.pull-right[

* Varying # of hidden layers with 128 neurons per layer


```r
## # A tibble: 8 x 3
##   nlayers min_loss train_time
##     &lt;int&gt;    &lt;dbl&gt;      &lt;dbl&gt;
## 1       1    5.959       6.88
## 2       2    0.824       4.32
## 3       3    0.609       4.32
## 4       4    0.750       4.54
## 5       5    0.665       4.36
## 6       6    0.998       4.43
## 7       7    0.802       4.45
## 8       8    0.603       4.78
```

]

---
class: clear, center, middle

background-image: url(https://www.refinitiv.com/content/dam/marketing/en_us/images/product/knowledge-direct-news-screenshot.jpg.transform/rect-768/q82/image.jpg)
background-size: cover

---
# Categorical crossentropy

.pull-left[

Remember our softmax activation function that produces the probability of each class?

&lt;img src="images/softmax.png" width="800" style="display: block; margin: auto;" /&gt;

]

.pull-right[

* Categorical crossentropy simply computes the log loss of the probability that the outcome is a particular class versus not being that class

* i.e.: `\(y = 3\)` vs `\(y \neq 3\)` 

* it just needs to sum up the probabilities of `\(y \neq 3\)`

&lt;br&gt;

`$$-\frac{1}{N} \sum^{N=C}_{i=1} {(y_i \cdot \log(p_i) + (1 - y_i) \cdot \log(1 - p_i))}$$`

]

---
# Sparse categorical crossentropy

.pull-left[

.bold[When your response is a vector of integers use sparse categorical crossentropy]





```r
str(train_labels)
##  int [1:8982] 3 4 3 4 4 4 4 3 3 16 ...
```

.bold[When your response is a one-hot encoded tensor use categorical crossentropy]





]

.pull-right[

* Categorical crossentropy simply computes the log loss of the probability that the outcome is a particular class versus not being that class

* i.e.: `\(y = 3\)` vs `\(y \neq 3\)` 
* it just needs to sum up the probabilities of `\(y \neq 3\)`

* .red[___sparse categorical crossentropy___]
   - mathematically the same
   - does not require the summation
   - requires less memory and is more efficient when dealing with ___many___ response categories (i.e. 10K)

]

---
# Weight regularization


---
# Dropout
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
