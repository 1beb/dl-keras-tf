---
title: "NLP: Transfer learning for Amazon review word embeddings"
output: html_notebook
---

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)

# Initialize package
library(keras)
library(tidyverse)
library(glue)
```


```{r}
amazon_reviews <- here::here("docs", "data", "amazon-food", "finefoods.txt")
reviews <- read_lines(amazon_reviews)
```

Different products

```{r}
products <- reviews[str_detect(reviews, "product/productId:")]
products <- str_sub(products, start = 19) %>% str_trim()

glue("There are {length(products)} total observations and ",
     "{n_distinct(products)} unique product reviews.")
```

```{r}
scores <- reviews[str_detect(reviews, "review/score:")]
scores <- str_extract(scores, "\\d.*") %>% as.numeric()

enframe(scores, name = "ID", value = "score") %>%
  count(score) %>%
  ggplot(aes(score, n)) +
  geom_col() +
  scale_y_continuous(labels = scales::comma)
```

Let's extract the text

```{r}
text <- reviews[str_detect(reviews, "review/text:")]
text <- str_replace(text, "review/text:", "")
text <- iconv(text, to = "UTF-8")

cat(crayon::blue("Do we have equal number of observations?\n"))

obs <- purrr::map_int(list(products, scores, text), length)
if (all.equal(length(unique(obs)), 1)) {
  cat(crayon::green(cli::symbol$tick), "All are equal!")
} else {
  cat(crayon::red(cli::symbol$tick), "We've got problems!")
}
```

Only care about those with strongly negative (1-2) or positive (5) reviews. 
Discard neutral reviews (3-4).

```{r}
index <- scores <= 2 | scores > 4
labels <- ifelse(scores[index] > 3, 1, 0)
text <- text[index]

testthat::expect_equal(length(text), length(labels))
glue("We have {sum(index)} total obs.")
glue("Positive: {sum(labels == 1)}, ({round(mean(labels == 1) * 100, 1)}%)")
glue("Negative: {sum(labels == 0)}, ({round(mean(labels == 0) * 100, 1)}%)")
```

```{r}
cat("First negative review:\n")
first_neg <- first(which(labels == 0))
text[first_neg]

cat("\nFirst positive review:\n")
first_pos <- first(which(labels == 1))
text[first_pos]
```

```{r}
text_df <- text %>%
  tibble(.name_repair = ~ "text") %>%
  mutate(text_length = str_trim(text) %>% str_count("\\w+"))

unique_words <- text_df %>%
  tidytext::unnest_tokens(word, text) %>%
  pull(word) %>%
  n_distinct()

avg_review_length <- median(text_df$text_length, na.rm = TRUE)
  
ggplot(text_df, aes(text_length)) +
  geom_histogram(bins = 100, fill = "grey70", color = "grey40") +
  geom_vline(xintercept = avg_review_length, color = "red", lty = "dashed") +
  scale_x_log10() +
  ggtitle(glue("Median review length is {avg_review_length}"),
          subtitle = glue("Total number of unique words is {unique_words}"))
```

Notice that there are reviews with very little information (i.e. word length < 10). 
Let's remove these and make sure that we only consider reviews that have 20 
words or more.

```{r}
preferred_length <- function(text, n_words = 20) {
   words_in_text <- text %>% str_trim() %>% str_count("\\w+")
   words_in_text >= n_words
}

index <- purrr::map_lgl(text, preferred_length)
text <- text[index]
labels <- labels[index]

testthat::expect_equal(length(text), length(labels))
glue("We now have {sum(index)} total obs.")
glue("Positive: {sum(labels == 1)}, ({round(mean(labels == 1) * 100, 1)}%)")
glue("Negative: {sum(labels == 0)}, ({round(mean(labels == 0) * 100, 1)}%)")
```


# Prepare data

```{r}
labels <- as.array(labels)
```

```{r}
top_n_words <- 10000

tokenizer <- text_tokenizer(num_words = top_n_words) %>% 
  fit_text_tokenizer(text)

names(tokenizer)
```

```{r}
word_index <- tokenizer$word_index
head(tokenizer$word_index)
```

Number of unique tokens:

```{r}
length(tokenizer$word_index)
```

`sequences` contains the vectorized values as a list.

```{r}
sequences <- texts_to_sequences(tokenizer, text)
```

```{r}
# The vectorized first instance:
sequences[[1]]
```

What the text has become:

```{r} 
cat(crayon::blue("Original text:\n"))
text[[1]]

cat(crayon::blue("\nRevised text:\n"))
paste(unlist(tokenizer$index_word)[sequences[[1]]] , collapse = " ")
```

We'll limit ourselves to the first XXX words.

```{r}
max_len <- 200
features <- pad_sequences(sequences, maxlen = max_len)
```

```{r}
features[1,]
```

```{r}
paste(unlist(tokenizer$index_word)[features[1,]], collapse = " ")
```

Shape of data tensor: `r dim(data)`, Shape of label tensor: `r dim(labels)`.

```{r}
dim(features)
dim(labels)
```

# Train model

```{r}
set.seed(123)
index <- sample(1:nrow(features))

features <- features[index, ]
labels <- labels[index]
```

```{r}
# Specify the maximum input length to the embedding layer so you can later 
# flatten the embedded inputs.
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = top_n_words, 
                  output_dim = 16,
                  input_length = max_len) %>%  # 100 length of each review.
  layer_flatten() %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)


history <- model %>% fit(
  features, labels,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2,
  callbacks = list(callback_early_stopping(patience = 2, 
                                           restore_best_weights = TRUE))
)
```

```{r}
wts <- get_weights(model)
embedding_wts <- wts[[1]]



embedding_wts <- embedding_wts %>% 
  as_tibble(.name_repair = "unique") %>%
  mutate(word = tokenizer$index_word[1:tokenizer$num_words] %>% unlist()) %>%
  select(word, everything())

embedding_wts
```

```{r}
write_csv(
  embedding_wts, 
  "../../docs/data/pretrained/amazon-reviews/embeddings.csv"
  )
```

