---
title: "NLP: Word embeddings to predict 'Helpfulness' of Amazon reviews"
output: html_notebook
---

The GloVE pretrained word embeddings are derived from the 2014 English Wikipedia. It can be downloaded [here](https://nlp.stanford.edu/projects/glove/). The file to look for is `glove.6B.zip` (822MB). In it you'll find  100-dimensional embedding vectors for 400,000 words (or non-word tokens).

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)

# Initialize package
library(keras)
library(tidyverse)
library(glue)

# helper functions we'll use to explore word embeddings
source("helper_functions.R")
```


```{r}
amazon_reviews <- here::here("docs", "data", "amazon-food", "finefoods.txt")
reviews <- read_lines(amazon_reviews)
```

Different products

```{r}
products <- reviews[str_detect(reviews, "product/productId:")]
products <- str_sub(products, start = 19) %>% str_trim()

glue("There are {length(products)} total observations and ",
     "{n_distinct(products)} unique product reviews.")
```

```{r}
scores <- reviews[str_detect(reviews, "review/score:")]
scores <- str_extract(scores, "\\d.*") %>% as.numeric()

glue("The scores range from {min(scores)} to {max(scores)} with an average of ",
     "{round(mean(scores), 2)}.")
```


Different levels of helpfulness (defined as fraction of users who found the 
review helpful)

```{r, fig.height=3.5}
helpfulness <- reviews[str_detect(reviews, "review/helpfulness:")]
helpfulness <- str_extract(helpfulness, "\\d.*")
num_reviews <- str_replace(helpfulness, "^.*\\/", "") %>% as.integer()
helpfulness <- str_replace(helpfulness, "\\/.*$", "") %>% as.integer()

tibble(num_reviews, helpfulness) %>%
  pivot_longer(
    num_reviews:helpfulness, 
    names_to = "category", 
    values_to = "count"
    ) %>%
  ggplot(aes(count)) +
  geom_histogram() +
  facet_wrap(~ category) + 
  scale_y_continuous(trans = "log1p")
```

Let's extract the text

```{r}
text <- reviews[str_detect(reviews, "review/text:")]
text <- str_replace(text, "review/text:", "")
text <- iconv(text, to = "UTF-8")

cat(crayon::blue("Do we have equal number of observations?\n"))

obs <- purrr::map_int(list(products, helpfulness, text), length)
if (all.equal(length(unique(obs)), 1)) {
  cat(crayon::green(cli::symbol$tick), "All are equal!")
} else {
  cat(crayon::red(cli::symbol$tick), "We've got problems!")
}
```


Only care about those with 10+ reviews

```{r}
num_index <- num_reviews >= 10
num_reviews <- num_reviews[num_index]
helpfulness <- helpfulness[num_index]
text <- text[num_index]

glue("There are {sum(num_index)} obs with over 10 reviews.")
```

Now define and find those reviews that have < 25% helpful score and > 25% helpful 
score.

```{r}
ratio <- helpfulness / num_reviews
ratio_index <- ratio <= 0.4 | ratio >= 0.6
labels <- ifelse(ratio[ratio_index] >= 0.5, 1, 0)
text <- text[ratio_index]

glue("We have {sum(ratio_index)} total obs.")
glue("Positive: {sum(labels == 1)}, ({round(mean(labels == 1) * 100, 1)}%)")
glue("Negative: {sum(labels == 0)}, ({round(mean(labels == 0) * 100, 1)}%)")
```


```{r}
cat("First negative review:\n")
first_neg <- first(which(labels == 0))
text[first_neg]

cat("\nFirst positive review:\n")
first_pos <- first(which(labels == 1))
text[first_pos]
```

```{r}
text_df <- text %>%
  tibble(.name_repair = ~ "text") %>%
  mutate(text_length = str_trim(text) %>% str_count("\\w+"))

unique_words <- text_df %>%
  tidytext::unnest_tokens(word, text) %>%
  pull(word) %>%
  n_distinct()

avg_review_length <- median(text_df$text_length, na.rm = TRUE)
  
ggplot(text_df, aes(text_length)) +
  geom_histogram(bins = 100, fill = "grey70", color = "grey40") +
  geom_vline(xintercept = avg_review_length, color = "red", lty = "dashed") +
  scale_x_log10() +
  ggtitle(glue("Median review length is {avg_review_length}"),
          subtitle = glue("Total number of unique words is {unique_words}"))
```

# Explore Glove Embeddings

```{r}
# clean up text and compute word embeddings
clean_text <- tolower(text) %>%
  str_replace_all(pattern = "[[:punct:] ]+", replacement = " ") %>%
  str_trim()

word_embeddings <- get_embeddings(clean_text)
```

Explore your own words!

```{r}
# find words with similar embeddings
get_similar_words("two", word_embeddings)
```

# Prepare data

```{r}
labels <- as.array(labels)
```

```{r}
top_n_words <- 20000

tokenizer <- text_tokenizer(num_words = top_n_words) %>% 
  fit_text_tokenizer(text)

names(tokenizer)
```

We have now tokenized the text, the first step in our process:

```{r}
word_index <- tokenizer$word_index
head(tokenizer$word_index)
```

Number of unique tokens:

```{r}
length(tokenizer$word_index)
```

`sequences` contains the vectorized values as a list.

```{r}
sequences <- texts_to_sequences(tokenizer, text)
```

```{r}
# The vectorized first instance:
sequences[[1]]
```

What the text has become:

```{r} 
cat(crayon::blue("Original text:\n"))
text[[1]]

cat(crayon::blue("\nRevised text:\n"))
paste(unlist(tokenizer$index_word)[sequences[[1]]] , collapse = " ")
```

We'll limit ourselves to the first 200 words.

```{r}
max_len <- 100
features <- pad_sequences(sequences, maxlen = max_len)
```

```{r}
features[1,]
```

```{r}
paste(unlist(tokenizer$index_word)[features[1,]], collapse = " ")
```

Shape of data tensor: `r dim(data)`, Shape of label tensor: `r dim(labels)`.

```{r}
dim(features)
dim(labels)
```


```{r}
set.seed(123)
index <- sample(1:nrow(features))

x_train <- features[index, ]
y_train <- labels[index]
```

```{r}
# Specify the maximum input length to the embedding layer so you can later flatten the embedded inputs.
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = top_n_words, 
                  output_dim = 32,
                  input_length = max_len) %>%  # 100 length of each review.
  layer_flatten() %>%
  layer_dense(units = 1, activation = "sigmoid")

summary(model)
```

Compile the model

```{r}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)
```

```{r}
history <- model %>% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2
)
```

```{r}
wts <- get_weights(model)
embedding_wts <- wts[[1]]
```

```{r}
words <- tibble(
  word = names(tokenizer$word_index), 
  id = as.integer(unlist(tokenizer$word_index))
)

words <- words %>%
  filter(id <= tokenizer$num_words) %>%
  arrange(id)

row.names(embedding_wts) <- words$word
```

```{r, message=FALSE}
library(text2vec)

find_similar_words <- function(word, embedding_wts, n = 6) {
  similarities <- embedding_wts[word, , drop = FALSE] %>%
    sim2(embedding_wts, y = ., method = "cosine")
  
  similarities[,1] %>% sort(decreasing = TRUE) %>% head(n)
}
```
 
```{r}
find_similar_words("delicious", embedding_wts, n = 10)
```